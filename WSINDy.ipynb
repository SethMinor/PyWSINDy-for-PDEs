{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SethMinor/WSINDy-for-Python/blob/main/WSINDy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qrx7eN3zlCd"
      },
      "source": [
        "## Weak SINDy Template\n",
        "##### Based on the [JCP paper by **D. A. Messenger**, **D. M. Bortz** 2021](https://www.sciencedirect.com/science/article/pii/S0021999121004204). <br> See authors' original [MatLab code repository](https://github.com/MathBioCU/WSINDy_PDE).\n",
        "##### Python code by Seth Minor, 2024."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzfbjzjX1NmK"
      },
      "outputs": [],
      "source": [
        "# Access your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directories\n",
        "%cd /content/drive/My Drive/WSINDy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7dIJ-kL78rZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import itertools\n",
        "\n",
        "# Symbolic derivatives\n",
        "%pip install symengine\n",
        "import symengine as sp\n",
        "\n",
        "# Parallel and GPU\n",
        "#import torch.nn.functional as nnF #GPU\n",
        "#from concurrent.futures import ProcessPoolExecutor, as_completed # Parallel\n",
        "\n",
        "# File path for empirical dataset\n",
        "file_path = '/content/drive/My Drive/WSINDy'\n",
        "data_path = file_path + '/example_dataset.txt'\n",
        "\n",
        "# Read in CSV data\n",
        "U = np.loadtxt(data_path, delimiter=',')\n",
        "\n",
        "# Convert to torch tensor\n",
        "U = torch.tensor(U)\n",
        "\n",
        "# If necessary, also reshape to Nx x ... x Nt tensor\n",
        "(Nx, Ny) = (200, 200) # Example\n",
        "U = U.view(Nx, Ny, -1)\n",
        "\n",
        "print(f\"Imported tensor with shape {U.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "wjPRmnVpblPk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "296iPuVwe1cX"
      },
      "source": [
        "#### Specify Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Two-piece linear approximation\n",
        "def two_piece(x, x0, y0, m1, m2):\n",
        "    return np.piecewise(x, [x<x0], [lambda x:m1*(x-x0)+y0, lambda x:m2*(x-x0)+y0])\n",
        "\n",
        "# Define the scalar-valued function F(m)\n",
        "def F(m,k,N,tau_hat,tau):\n",
        "  log_term = np.log((2*m-1)/m**2)\n",
        "  mid_term = (2*np.pi*k*m)**2 - 3*(tau_hat*N)**2\n",
        "  last_term = 2*(tau_hat*N)**2 * np.log(tau)\n",
        "  return log_term * mid_term - last_term\n",
        "\n",
        "def optimal_support(U, d, two_piece, F, tau, tau_hat, **kwargs):\n",
        "  # kwargs = {verbosity, init_guess}\n",
        "  #-------------------------------------------\n",
        "  # verbosity = create plots? (0 or 1)\n",
        "  # init_guess = two-piece curve fitting guess\n",
        "  #-------------------------------------------\n",
        "\n",
        "  # Check kwargs\n",
        "  if 'verbosity' in kwargs:\n",
        "    verbosity = kwargs['verbosity']\n",
        "  else:\n",
        "    verbosity = 0\n",
        "  if 'init_guess' in kwargs:\n",
        "    init_guess = kwargs['init_guess']\n",
        "  elif d < (U.dim()-1):\n",
        "    init_guess = [15,1,10,0] # Space\n",
        "  else:\n",
        "    init_guess = [3,1,10,0.25] # Time\n",
        "\n",
        "  # FFT of U in one variable (e.g., x)\n",
        "  Uhat_d = abs(torch.fft.rfft(U, n = U.shape[d], dim = d))\n",
        "\n",
        "  # Average over others (e.g., t and y)\n",
        "  dimmers = [dim for dim in range(Uhat_d.ndimension()) if dim != d]\n",
        "  Uhat_d = Uhat_d.mean(dim = dimmers)\n",
        "\n",
        "  # Cumulative sum\n",
        "  Hd = torch.cumsum(Uhat_d, dim = 0)\n",
        "  Hd = (Hd/Hd.max()).numpy() # Normalize for curve fitting\n",
        "\n",
        "  # Run curve-fittting routine\n",
        "  freqs = torch.arange(0, np.floor(U.shape[d]/2)+1, 1).numpy()\n",
        "  params = scipy.optimize.curve_fit(two_piece, freqs, Hd, p0=init_guess)[0]\n",
        "\n",
        "  # Grab critical wavenumber\n",
        "  k = int(params[0])\n",
        "  N = U.shape[d]\n",
        "\n",
        "  # Redefine init_guess for the root finder\n",
        "  init_guess = (np.sqrt(3)*N*tau_hat)/(2*np.pi*k)\n",
        "  init_guess = init_guess * (1 + np.sqrt(1 - (8/np.sqrt(3))*np.log(tau)))/2\n",
        "\n",
        "  # Use the root function to find the root\n",
        "  md = int(scipy.optimize.root(F, init_guess, args=(k,N,tau_hat,tau)).x[0])\n",
        "\n",
        "  if verbosity == 1:\n",
        "    # Print optimal support value\n",
        "    print(f'Optimal test function support is m={md} (d={d+1}).')\n",
        "\n",
        "    # Plot the critical wavenumber\n",
        "    plt.style.use('_mpl-gallery')\n",
        "    plt.rcParams['figure.figsize'] = [6, 2]\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(freqs, Uhat_d, 'r--', label='$\\mathcal{F}[U]$')\n",
        "    ax.plot(freqs, Uhat_d.max()*Hd, 'k', label='$H^d$ (rescaled)')\n",
        "    ax.plot(freqs,Uhat_d.max()*two_piece(freqs,*params),'g--',label='Piecewise Approx')\n",
        "    ax.plot(params[0], Uhat_d.max()*params[1],'go')\n",
        "    ax.set_xlabel('$k$')\n",
        "    ax.set_title(f'Critical Wavenumber, $k_d=${k} ($d=${d+1})')\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.show()\n",
        "\n",
        "  # Return optimal support\n",
        "  return md"
      ],
      "metadata": {
        "id": "0NNyO0Yx2Y_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY5Sn6AqYwT9"
      },
      "outputs": [],
      "source": [
        "# Coordinate subsampling (x, y, or t)\n",
        "def uniform_subsample(s, m, x):\n",
        "  # Check size compatability\n",
        "  if (2*m + 1) > x.shape[0]:\n",
        "    raise Exception('Error: m produces non-compact support.')\n",
        "\n",
        "  # Subsample\n",
        "  xk = x[m:-m:s]\n",
        "\n",
        "  # Compute indices of query points along given dimension\n",
        "  indices = (x.unsqueeze(0) == xk.unsqueeze(1)).nonzero(as_tuple=True)[1]\n",
        "  return indices.tolist()\n",
        "\n",
        "# Apply 'mask' of subsampled query point indices to a given matrix\n",
        "# (E.g., U[mask_rows, mask_cols] = Uk, as a vector)\n",
        "def query_mask(s, m, X, D, **kwargs):\n",
        "  # kwargs = {verbosity}\n",
        "  #-------------------------------------------\n",
        "  # verbosity = create plots? (0 or 1)\n",
        "  #-------------------------------------------\n",
        "\n",
        "  # Compute mask of query point indices\n",
        "  subsamples = [uniform_subsample(s[i % len(s)], m[i % len(m)], X[i % len(X)]) for i in range(D+1)]\n",
        "  my_mask = list(itertools.product(*subsamples))\n",
        "\n",
        "  # Return mask\n",
        "  mask = tuple(zip(*my_mask))\n",
        "\n",
        "  # Optional plotting of query points\n",
        "  # GENERALIZE THIS!!\n",
        "  if ('verbosity' in kwargs) and (kwargs['verbosity'] == 1):\n",
        "    if D == 2:\n",
        "      X_mesh, Y_mesh, T_mesh = torch.meshgrid(X[0], X[1], X[-1], indexing='ij')\n",
        "      Xk_mesh, Yk_mesh, Tk_mesh = torch.meshgrid(X[0][m[0]:-m[0]:s[0]],\n",
        "                                                 X[1][m[1]:-m[1]:s[1]],\n",
        "                                                 X[-1][m[-1]:-m[-1]:s[-1]],\n",
        "                                                 indexing='ij')\n",
        "\n",
        "      plt.style.use('_mpl-gallery')\n",
        "      plt.rcParams['figure.figsize'] = [4, 4]\n",
        "      fig, ax = plt.subplots()\n",
        "      ax.pcolormesh(X_mesh[:,:,-1], Y_mesh[:,:,-1], U[:,:,-1])\n",
        "      ax.scatter(Xk_mesh, Yk_mesh, color='red', s=3)\n",
        "      ax.set_title('Query Points')\n",
        "      ax.set_xlabel('$x$')\n",
        "      ax.set_ylabel('$y$')\n",
        "      print(f'\\nQuery Points')\n",
        "      plt.show()\n",
        "\n",
        "  # Return\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lr29C1Uebcm"
      },
      "source": [
        "#### Create Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFU9bU4nyTSJ"
      },
      "outputs": [],
      "source": [
        "# Calculate symbolic derivatives\n",
        "\n",
        "def D_phibar(x, D, x_sym, phi_bar):\n",
        "  # Dth derivative at degree p\n",
        "  D_phi = sp.diff(phi_bar, x_sym, D)\n",
        "\n",
        "  # Evaluate at point x\n",
        "  if abs(x) < 1:\n",
        "    return float(D_phi.subs(x_sym, x))\n",
        "  else:\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkVvCrcNekM1"
      },
      "outputs": [],
      "source": [
        "def get_test_fcns(m, tau, x, alpha, d, D, **kwargs):\n",
        "  # kwargs = {dx, p, scales, verbosity}\n",
        "  #----------------------------------------\n",
        "  # dx = space or time discretization\n",
        "  # p = known degree for test fcns\n",
        "  # scales = scale factors (yu, yx, yt)\n",
        "  # verbosity = create plots? (0 or 1)\n",
        "  #----------------------------------------\n",
        "\n",
        "  # Number of points in x_d coordinate\n",
        "  N = x.shape[0]\n",
        "\n",
        "  # Check if dx (or dt) were provided\n",
        "  if 'dx' in kwargs:\n",
        "    dx = kwargs['dx']\n",
        "  else:\n",
        "    dx = (x[1] - x[0]).item()\n",
        "\n",
        "  # Check if scaling factors were provided\n",
        "  # ASSUMES IDENTICAL SPATIAL SCALING\n",
        "  if 'scales' in kwargs:\n",
        "    (yu, yx, yt) = kwargs['scales']\n",
        "    d_scale = (D*(yx,) + (yt,))[d]\n",
        "    dx = d_scale * dx\n",
        "\n",
        "  # Check size compatibility of support m\n",
        "  if (m > (N-1)/2) or (m <= 1):\n",
        "    raise Exception('Error: test fcn not compactly supported.')\n",
        "\n",
        "  # Check if a known p was provided\n",
        "  if 'p' in kwargs:\n",
        "    p = kwargs['p']\n",
        "  else:\n",
        "    # Solve minimization problem for degree p\n",
        "    alpha_bar = max(tuple(item[d] for item in alpha))\n",
        "    log_tau_term = np.ceil(np.log(tau)/np.log((2*m-1)/m**2))\n",
        "    p = max(log_tau_term, alpha_bar + 1)\n",
        "\n",
        "  # Initialize grid of discretized test fcn values\n",
        "  test_fcns_d = torch.zeros(len(alpha), 2*m+1, dtype = x.dtype)\n",
        "  n_grid = torch.arange(-1, 1.0001, 1/m, dtype = x.dtype)\n",
        "\n",
        "  # Multi-index values (a_d^i), for i=0,...,S\n",
        "  multi_index_d = tuple(item[d] for item in alpha)\n",
        "\n",
        "  # Precompute symbolic derivative variables\n",
        "  x_sym = sp.Symbol('x')\n",
        "  phi_bar = (1 - x_sym**2)**p\n",
        "  vec = np.vectorize(D_phibar)\n",
        "\n",
        "  # Compute D^i derivative of phi_d, for i=0,...,S\n",
        "  for i in range(len(alpha)):\n",
        "    # Speed-up: check for repeated values\n",
        "    if (i > 0) and (multi_index_d[i-1] == multi_index_d[i]):\n",
        "      test_fcns_d[i,:] += test_fcns_d[i-1,:]\n",
        "    else:\n",
        "      # Get multi-index value (a_d^i)\n",
        "      num_derivs = multi_index_d[i]\n",
        "\n",
        "      # Evaluate the (a_d^i) derivative on n_grid\n",
        "      A_i = torch.from_numpy(vec(n_grid, num_derivs, x_sym, phi_bar))\n",
        "\n",
        "      # Add the rescaled D^i derivative to the grid of values\n",
        "      test_fcns_d[i,:] += (1/((m*dx)**num_derivs)) * A_i\n",
        "\n",
        "  # Optional plotting of some test functions\n",
        "  if ('verbosity' in kwargs) and (kwargs['verbosity'] == 1) and (d == 0):\n",
        "    # Scaled test function grid\n",
        "    y_grid = d_scale* torch.arange(-m*dx, m*dx+0.0001, dx)\n",
        "\n",
        "    plt.style.use('_mpl-gallery')\n",
        "    plt.rcParams['figure.figsize'] = [6, 2]\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    ax.plot(y_grid, test_fcns_d[1,:], label = '$\\mathcal{D}^1$')\n",
        "    ax.plot(y_grid, test_fcns_d[2,:], label = '$\\mathcal{D}^2$')\n",
        "    ax.plot(y_grid, test_fcns_d[3,:], label = '$\\mathcal{D}^3$')\n",
        "    ax.plot(y_grid, test_fcns_d[4,:], label = '$\\mathcal{D}^4$')\n",
        "\n",
        "    ax.set_title(f'A Few Test Functions $(d={d})$')\n",
        "    ax.set_xlabel('$y = x_k - x$')\n",
        "    ax.set_ylabel('${\\phi_d}^{(a_d^i)} (y)$')\n",
        "    plt.legend(loc = 'best')\n",
        "    print(f'\\nTest Functions')\n",
        "    plt.show()\n",
        "\n",
        "  # Return test function derivatives\n",
        "  return test_fcns_d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Speedy Test Function Creation\n",
        "# Checks for and exploits redundancy in derivative library.\n",
        "\n",
        "def speedy_test_fcns(m, p, tau, X, dX, alpha, D, **kwargs):\n",
        "  # kwargs = {scales, verbosity}\n",
        "  #------------------------------------\n",
        "  # scales = scale factors (yu, yx, yt)\n",
        "  # verbosity = create plots? (0 or 1)\n",
        "  #------------------------------------\n",
        "\n",
        "  # Initialize test functions\n",
        "  test_fcns = []\n",
        "\n",
        "  # The speed-up only makes sense for D>1 and equal spatial m,p values\n",
        "  m_check = all([m[0] == m[n] for n in range(D)])\n",
        "  p_check = all([p[0] == p[n] for n in range(D)])\n",
        "\n",
        "  # Checking whether it makes sense to try the speed-up\n",
        "  if (D == 1) or (m_check * p_check == False):\n",
        "    go_condition = False\n",
        "  else:\n",
        "    try:\n",
        "      # Building a condition to indicate if alpha is listed sequentially\n",
        "      (a0, a1) = ([elem[0] for elem in alpha], [elem[1] for elem in alpha])\n",
        "      a_max = max(a0)\n",
        "      go_condition = (a0[2:2+a_max] == a1[2+a_max:2+2*a_max])\n",
        "    except:\n",
        "      # In case this throws an error due to a weird alpha structure\n",
        "      print(f'Derivative library not given in a symmetric, sequential order.')\n",
        "      go_condition = False\n",
        "\n",
        "  # If the conditions aren't right, return the usual slow computation\n",
        "  if (go_condition == False):\n",
        "    print(f'\\nUsing slow test function method...\\n')\n",
        "    for d in range(D + 1):\n",
        "      (kwargs['dx'], kwargs['p']) = (dX[d], p[d])\n",
        "      test_fcns.append(get_test_fcns(m[d], tau, X[d], alpha, d, D, **kwargs))\n",
        "    return test_fcns\n",
        "\n",
        "  # Otherwise, perform fast computation exploiting redundant derivatives\n",
        "  else:\n",
        "    # Index after which mixed/'cross' derivatives would start (if present)\n",
        "    c_i = 2 + D*a_max\n",
        "\n",
        "    # Maximum cross derivative\n",
        "    if c_i < len(alpha):\n",
        "      c_max = max([elem[0] for elem in alpha[c_i:]])\n",
        "\n",
        "    # Define a permutation that allows computation of step (d+1) from step (d)\n",
        "    # This takes indices [0,1,...,S-1] --> [block_1, block_2, block_3]\n",
        "    normal_inds = [ind for ind in range(len(alpha))]\n",
        "\n",
        "    # Block 1: the first two indices are fixed (D0 and D1)\n",
        "    block_1 = normal_inds[0:2]\n",
        "\n",
        "    # Block 2: pure derivative terms are shifted cyclically\n",
        "    block_2 = normal_inds[2:c_i]\n",
        "    block_2 = block_2[-a_max:] + block_2[0:-a_max]\n",
        "\n",
        "    # Block 3: cross derivative terms are 'cut' and 'stretched'\n",
        "    # (Note: this permutation is dependent of the value of 'd'.)\n",
        "    block_3 = normal_inds[c_i:]\n",
        "    if block_3 == []:\n",
        "      # No cross derivatives are present\n",
        "      block_3 = [[] for d in range(D-1)]\n",
        "    else:\n",
        "      # Cut and stretch the first '(d+1)*cross_max' variables over 'block_3'\n",
        "      b3 = c_max**(D-2)\n",
        "      block_3 = [(b3//(d+1))*[c_max*[ind] for ind in block_3[0:(d+1)*c_max]] for d in range(D-1)]\n",
        "      block_3 = [[ind for elem in block_3[d] for ind in elem] for d in range(D-1)]\n",
        "\n",
        "    # Compute the first and last test function\n",
        "    tf_1 = get_test_fcns(m[0], tau, X[0], alpha, 0, D, **kwargs)\n",
        "    test_fcns.append(tf_1)\n",
        "    tf_d = tf_1 # The d^th test function\n",
        "    tf_t = get_test_fcns(m[-1], tau, X[-1], alpha, D, D, **kwargs)\n",
        "\n",
        "    # Use permuted indices to automatically compute the other test functions\n",
        "    for d in range(1, D):\n",
        "      perm_inds = block_1 + block_2 + block_3[d-1]\n",
        "      tf_d = tf_d[perm_inds,:]\n",
        "\n",
        "      # Rescale if needed\n",
        "      if dX[0] != dX[d]:\n",
        "        if d == 1:\n",
        "          ad = a0\n",
        "        elif d == 2:\n",
        "          ad = a1\n",
        "        else:\n",
        "          ad = [elem[d-1] for elem in alpha]\n",
        "        ad = torch.tensor(ad, dtype=tf_d.dtype).view(-1, 1)\n",
        "        tf_d *= (dX[d-1]/dX[d])**ad\n",
        "\n",
        "      # Add to list of test functions\n",
        "      test_fcns.append(tf_d)\n",
        "\n",
        "    # Add the temporal test function and return the list\n",
        "    test_fcns.append(tf_t)\n",
        "    return test_fcns"
      ],
      "metadata": {
        "id": "SPa7CTJYColD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMtEJdjyYah7"
      },
      "source": [
        "#### Rescaling the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKB9SudTktiD"
      },
      "outputs": [],
      "source": [
        "# Carefully compute n-choose-k with non-integer k\n",
        "def my_nchoosek(n,k):\n",
        "  n_factorial = scipy.special.factorial(n)\n",
        "  k_factorial = scipy.special.factorial(np.ceil(k))\n",
        "  nk_term = scipy.special.factorial(n-np.floor(k))\n",
        "  return n_factorial / (nk_term * k_factorial)\n",
        "\n",
        "# Expects U as a list, [U1,...,Un]\n",
        "# THIS IS CURRENTLY SET UP FOR IDENTICAL X AND Y\n",
        "\n",
        "def get_scales(fj, alpha, U, m, dX, p, **kwargs):\n",
        "  # kwargs = {scales}\n",
        "  #-----------------------------------\n",
        "  # scales = known scales (yu, yx, yt)\n",
        "  #-----------------------------------\n",
        "\n",
        "  # Check if known scales were provided\n",
        "  if ('scales' in kwargs):\n",
        "    # For multiple fields, yu should be a list\n",
        "    (yu, yx, yt) = kwargs['scales']\n",
        "\n",
        "  # Otherwise, compute scales\n",
        "  else:\n",
        "    # Maximum monomial power in library\n",
        "    beta_bar = 0\n",
        "    for n in range(len(U)):\n",
        "      max_n = max(tuple(item[n] for item in fj['poly']))\n",
        "      beta_bar = max(beta_bar, max_n)\n",
        "\n",
        "    # Find maximum spatial derivative\n",
        "    alpha_bar_x = 0\n",
        "    for d in range(len(alpha[0]) - 1):\n",
        "      max_d = max(tuple(item[d] for item in alpha))\n",
        "      alpha_bar_x = max(alpha_bar_x, max_d)\n",
        "\n",
        "    # Find maximum temporal derivative\n",
        "    alpha_bar_t = max(tuple(item[-1] for item in alpha))\n",
        "\n",
        "    # Compute L^2 norm of U and U^beta as vectors\n",
        "    U_2 = [torch.linalg.norm(torch.reshape(u, (-1,))).item() for u in U]\n",
        "    U_beta = [torch.linalg.norm(torch.reshape(u**beta_bar, (-1,))).item() for u in U]\n",
        "    yu = [(U_2[n]/U_beta[n])**(1/beta_bar) for n in range(len(U))]\n",
        "\n",
        "    # Compute scales using ansatz given in paper\n",
        "    # ASSUMES IDENTICAL SPATIAL RESCALING\n",
        "    yx = (1/(m[0]*dX[0])) * (my_nchoosek(p[0], alpha_bar_x/2)\n",
        "          *scipy.special.factorial(alpha_bar_x))**(1/alpha_bar_x)\n",
        "    yt = (1/(m[-1]*dX[-1])) * (my_nchoosek(p[-1], alpha_bar_t/2)\n",
        "          *scipy.special.factorial(alpha_bar_t))**(1/alpha_bar_t)\n",
        "\n",
        "  # Compute scale matrix, M = diag(mu)\n",
        "  # Dimensions are K x SJ where J=beta_bar^n and n=no. of fields\n",
        "  mu = torch.zeros((len(alpha)-1) * len(fj['poly']), dtype=U[0].dtype)\n",
        "\n",
        "  for j in range(len(fj['poly'])):\n",
        "    for i in range(1, len(alpha)):\n",
        "\n",
        "      # Exclude derivatives of constants\n",
        "      if (j == 0) and (i > 1):\n",
        "        pass\n",
        "      else:\n",
        "        # Exponents for yx, yt\n",
        "        x_exp = 0\n",
        "        for d in range(len(alpha[0])-1):\n",
        "          x_exp += alpha[i][d] - alpha[0][d]\n",
        "        t_exp = alpha[i][-1] - alpha[0][-1]\n",
        "\n",
        "        # GENERALIZE FOR TRIG FCNS, ETC.\n",
        "        # Take product of u-scales\n",
        "        yu_term = [yu[n]**(1-fj['poly'][j][n]) for n in range(len(U))]\n",
        "        yu_term = np.prod(yu_term)\n",
        "\n",
        "        # Set corresponding mu value\n",
        "        mu[(i-1)*len(fj['poly']) + j] = yu_term * yx**x_exp * yt**t_exp\n",
        "\n",
        "  # Make diagonal matrix\n",
        "  M = torch.diag(mu)\n",
        "\n",
        "  # Remove skipped columns\n",
        "  # AUTOMATE THIS TO WORK WITH ANY FJ, ALPHA\n",
        "  cols_to_remove = [len(fj['poly'])*c for c in range(1,len(alpha)-1)]\n",
        "  Lib_mask = torch.ones(M.shape[1], dtype=torch.bool)\n",
        "  Lib_mask[cols_to_remove] = False\n",
        "  M = torch.diag(M[Lib_mask, Lib_mask])\n",
        "\n",
        "  # Return scales and scale matrix\n",
        "  return yu, yx, yt, M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p14j807DZRsF"
      },
      "source": [
        "#### Create the LHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk3kfPRut3SU"
      },
      "outputs": [],
      "source": [
        "# Compute the LHS vector b\n",
        "def create_b(U, test_fcns, dX, mask, D, **kwargs):\n",
        "  # kwargs = {scales}\n",
        "  #----------------------------------------\n",
        "  # scales = scale factors ([yu's], yx, yt)\n",
        "  #----------------------------------------\n",
        "\n",
        "  # Check if scaling factors were provided\n",
        "  # ASSUMES IDENTICAL SPATIAL SCALING\n",
        "  if 'scales' in kwargs:\n",
        "    (yu, yx, yt) = kwargs['scales']\n",
        "    (U_, dX_) = (yu*U, [yx*dx for dx in dX[0:-1]]+[yt*dX[-1]])\n",
        "  else:\n",
        "    (U_, dX_) = (U, dX)\n",
        "\n",
        "  # Temporarily convert test functions to numpy arrays\n",
        "  test_fcns_ = [test_fcn.numpy() for test_fcn in test_fcns]\n",
        "  U_ = U_.numpy()\n",
        "\n",
        "  # Convolve test function derivatives with data\n",
        "  conv = U_\n",
        "  for d in range(D+1):\n",
        "    # Reshape test function appropriately\n",
        "    slicing = [None] * len(test_fcns)\n",
        "    slicing[d] = slice(None)\n",
        "\n",
        "    # 1D convolution along the d-th axis\n",
        "    conv = scipy.signal.convolve(conv, test_fcns_[d][tuple(slicing)], mode='same')\n",
        "\n",
        "  # Convert back to pytorch tensor\n",
        "  b = np.prod(dX_)*(torch.from_numpy(conv))\n",
        "\n",
        "  # Convert to column vector and return\n",
        "  b = (b[mask]).reshape(-1,1)\n",
        "  return b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XjeC_7wGVEv"
      },
      "source": [
        "#### Create the Model Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buV2tnmTGdGF"
      },
      "outputs": [],
      "source": [
        "# Evaluate library functions {fj} on data\n",
        "# (Note: j is 1-indexed here, so input j=1,...,J)\n",
        "\n",
        "# Expects U as a list, U=[U1,...,Un]\n",
        "# If aux fields exist, expects U=[...,V1,...,Vm]\n",
        "\n",
        "def evaluate_fj(name, fj, j, U):\n",
        "  # Monomial function for any number of fields (U1,...,Un)\n",
        "  if name == 'poly':\n",
        "    # Evaluate jth monomial, e.g., (u1)^3*(u2)^2\n",
        "    fj_of_U_ = [(U[n]**fj[name][j-1][n]).numpy() for n in range(len(U))]\n",
        "    fj_of_U_ = torch.from_numpy(np.prod(fj_of_U_, axis=0))\n",
        "\n",
        "  # Sinus/cosinus for a single field u(x,t)\n",
        "  # NOT YET IMPLEMENTED FOR MULTIPLE FIELDS\n",
        "  elif name == 'trig':\n",
        "    raise Exception('Trig functions not currently supported!')\n",
        "    # Evaluate jth frequency [0], phase [1]\n",
        "    #fj_of_U_ = np.sin(fj[name][j-1][0]*U + fj[name][j-1][1])\n",
        "\n",
        "  # In case the given function is not defined\n",
        "  else:\n",
        "    raise Exception(f'Function \"{name}\" was not found.')\n",
        "\n",
        "  # Return fj evaluated on data\n",
        "  return fj_of_U_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute column of model library\n",
        "# (Evaluated for specific i,j)\n",
        "\n",
        "# Expects U as a list, U=[U1,...,Un]\n",
        "# If aux fields exist, expects U=[...,V1,...,Vm]\n",
        "\n",
        "def get_library_column(fj_of_U, test_fcns, dX, mask, D, **kwargs):\n",
        "  # kwargs = {scales}\n",
        "  #----------------------------------------\n",
        "  # scales = scale factors ([yu's], yx, yt)\n",
        "  #----------------------------------------\n",
        "\n",
        "  # Check if scaling factors were provided\n",
        "  # ASSUMES IDENTICAL SPATIAL SCALING\n",
        "  if 'scales' in kwargs:\n",
        "    (yu, yx, yt) = kwargs['scales']\n",
        "    dX_ = [yx*dx for dx in dX[0:-1]] + [yt*dX[-1]]\n",
        "  else:\n",
        "    dX_ = dX\n",
        "\n",
        "  # Temporarily convert test functions to numpy arrays\n",
        "  test_fcns_ = [test_fcn.numpy() for test_fcn in test_fcns]\n",
        "  fj_of_U_ = fj_of_U.numpy()\n",
        "\n",
        "  # Convolve test function derivatives with data\n",
        "  conv = fj_of_U_\n",
        "  for d in range(D+1):\n",
        "    # Reshape test function appropriately\n",
        "    slicer = [None] * len(test_fcns)\n",
        "    slicer[d] = slice(None)\n",
        "\n",
        "    # 1D convolution along the d-th axis\n",
        "    conv = scipy.signal.convolve(conv, test_fcns_[d][tuple(slicer)], mode='same')\n",
        "\n",
        "  # Convert back to pytorch tensor\n",
        "  Lij_matrix = np.prod(dX_)*(torch.from_numpy(conv))\n",
        "\n",
        "  # Convert to column vector over query points and return\n",
        "  Lij = (Lij_matrix[mask]).reshape(-1,1)\n",
        "  return Lij"
      ],
      "metadata": {
        "id": "Y-hUtFrMB9So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the entire model library L\n",
        "# Expects U as a list, U=[U1,...,Un]\n",
        "\n",
        "def create_L(U, test_fcns, dX, mask, D, alpha, fj, **kwargs):\n",
        "  # kwargs = {scales, aux_fields}\n",
        "  #-------------------------------------------------\n",
        "  # scales = scale factors ([yu's], yx, yt)\n",
        "  # aux_fields = extra library variables [V1,...,Vm]\n",
        "  #-------------------------------------------------\n",
        "\n",
        "  #Initialize library variables\n",
        "  L_kwargs = {}\n",
        "  U_ = U\n",
        "\n",
        "  # Check if scaling factors were provided\n",
        "  # ASSUMES IDENTICAL SPATIAL SCALING\n",
        "  if 'scales' in kwargs:\n",
        "    (yu, yx, yt) = kwargs['scales']\n",
        "    L_kwargs['scales'] = (yu, yx, yt)\n",
        "\n",
        "    # Rescale each field\n",
        "    for n in range(len(yu)):\n",
        "      U_[n] *= yu[n]\n",
        "\n",
        "  # Check for extra variables\n",
        "  if 'aux_fields' in kwargs:\n",
        "    aux_fields = kwargs['aux_fields']\n",
        "  else:\n",
        "    aux_fields = []\n",
        "  U_ += aux_fields\n",
        "\n",
        "  # Create function names\n",
        "  fj_names = ['poly']*len(fj['poly']) + ['trig']*len(fj['trig'])\n",
        "\n",
        "  # The library is a K x SJ matrix\n",
        "  (K, S, J) = (len(mask[0]), len(alpha)-1, sum(len(fcn) for fcn in fj.values()))\n",
        "  L = torch.zeros(K, S*J, dtype=U[0].dtype)\n",
        "\n",
        "  # Loop over all functions {fj}\n",
        "  for j in range(1, J+1):\n",
        "    # Compute fj(U)\n",
        "    name = fj_names[j-1]\n",
        "    fj_of_U = evaluate_fj(name, fj, j, U_)\n",
        "\n",
        "    # Loop over all derivatives D^i\n",
        "    for i in range(1, S+1):\n",
        "      # Exclude derivatives of constants\n",
        "      # (These are the D^i[f_1] for i=2,...,S terms)\n",
        "      if (name == 'poly') and (j == 1) and (i > 1):\n",
        "        pass\n",
        "      else:\n",
        "        # Get the corresponding test functions\n",
        "        test_fcns_i = [test_fcn[i,:] for test_fcn in test_fcns]\n",
        "\n",
        "        # Compute corresponding library column\n",
        "        fj_of_U_ = fj_of_U.clone()\n",
        "\n",
        "        # Check if GPU is available\n",
        "        # GENERALIZE GPU CONVS TO ARB DIMENSION\n",
        "        if torch.cuda.is_available() and (D < 3):\n",
        "          Lij = get_library_column_gpu(fj_of_U_, test_fcns_i, dX, mask, D, **L_kwargs)\n",
        "        else:\n",
        "          Lij = get_library_column(fj_of_U_, test_fcns_i, dX, mask, D, **L_kwargs)\n",
        "        L[:, (i-1)*J + j-1] = Lij[:, 0]\n",
        "\n",
        "  # Remove skipped columns\n",
        "  cols_to_remove = [len(fj['poly'])*c for c in range(1, S)]\n",
        "  Lib_mask = torch.ones(L.shape[1], dtype=torch.bool)\n",
        "  Lib_mask[cols_to_remove] = False\n",
        "  L = L[:, Lib_mask]\n",
        "\n",
        "  # Return complete model library\n",
        "  return L"
      ],
      "metadata": {
        "id": "12nCc6W6lwpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying `get_library_column` on the GPU"
      ],
      "metadata": {
        "id": "64xsXW9BFEB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expects U as a list, U=[U1,...,Un]\n",
        "# If aux fields exist, expects U=[...,V1,...,Vm]\n",
        "\n",
        "def get_library_column_gpu(fj_of_U, test_fcns, dX, mask, D, **kwargs):\n",
        "  # kwargs = {scales}\n",
        "  #----------------------------------------\n",
        "  # scales = scale factors ([yu's], yx, yt)\n",
        "  #----------------------------------------\n",
        "\n",
        "  # Check if scaling factors were provided\n",
        "  # ASSUMES IDENTICAL SPATIAL SCALING\n",
        "  if 'scales' in kwargs:\n",
        "    (yu, yx, yt) = kwargs['scales']\n",
        "    dX_ = [yx*dx for dx in dX[0:-1]] + [yt*dX[-1]]\n",
        "  else:\n",
        "    dX_ = dX\n",
        "\n",
        "  # Move tensors to GPU if available\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  fj_of_U = fj_of_U.to(device)\n",
        "  test_fcns = [test_fcn.to(device) for test_fcn in test_fcns]\n",
        "\n",
        "  # Convert test functions to appropriate PyTorch format\n",
        "  conv = fj_of_U.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "\n",
        "  for d in range(D+1):\n",
        "    # Reshape test function appropriately for 3D convolution\n",
        "    test_fcn = test_fcns[d]\n",
        "    slicer = [1, 1] + [1] * fj_of_U.dim()\n",
        "    slicer[d+2] = -1\n",
        "    test_fcn = test_fcn.view(*slicer)\n",
        "\n",
        "    # Select appropriate convolution function\n",
        "    # ADD GENERAL CASE\n",
        "    if fj_of_U.dim() == 1:\n",
        "      conv = nnF.conv1d(conv, test_fcn, padding='same')\n",
        "    elif fj_of_U.dim() == 2:\n",
        "      conv = nnF.conv2d(conv, test_fcn, padding='same')\n",
        "    elif fj_of_U.dim() == 3:\n",
        "      conv = nnF.conv3d(conv, test_fcn, padding='same')\n",
        "    else:\n",
        "      raise ValueError(f\"Unsupported dimension: {fj_of_U.dim()}\")\n",
        "\n",
        "  # Remove batch and channel dimensions\n",
        "  conv = conv.squeeze(0).squeeze(0)\n",
        "\n",
        "  # Compute the scaling factor\n",
        "  Lij_matrix = torch.prod(torch.tensor(dX_, device=device)) * conv\n",
        "  Lij_matrix = Lij_matrix.cpu()\n",
        "\n",
        "  # Convert to column vector over query points and return\n",
        "  Lij = Lij_matrix[mask].reshape(-1, 1)\n",
        "\n",
        "  return Lij"
      ],
      "metadata": {
        "id": "rfhxR5NrFHmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to get this to run in parallel."
      ],
      "metadata": {
        "id": "xC2Rmvrqj3fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your evaluate_fj and get_library_column functions\n",
        "\n",
        "def parallel_worker(i, j, fj_of_U, name, test_fcns, dX, mask, D, L_kwargs):\n",
        "  if (name == 'poly') and (j == 1) and (i > 1):\n",
        "    return None, (i, j)\n",
        "  else:\n",
        "    test_fcns_i = [test_fcn[i,:] for test_fcn in test_fcns]\n",
        "    Lij = get_library_column(fj_of_U, test_fcns_i, dX, mask, D, **L_kwargs)\n",
        "    return Lij[:, 0], (i, j)"
      ],
      "metadata": {
        "id": "G4qOUDTKkIKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the entire model library L\n",
        "\n",
        "# Expects U as a list, U=[U1,...,Un]\n",
        "# If aux fields exist, expects U=[...,V1,...,Vm]\n",
        "\n",
        "def create_L_parallel(U, test_fcns, dX, mask, D, alpha, fj, **kwargs):\n",
        "  # kwargs = {scales, aux_fields}\n",
        "  #-------------------------------------------------\n",
        "  # scales = scale factors ([yu's], yx, yt)\n",
        "  # aux_fields = extra library variables [V1,...,Vm]\n",
        "  #-------------------------------------------------\n",
        "\n",
        "  #Initialize library variables\n",
        "  L_kwargs = {}\n",
        "  U_ = U\n",
        "\n",
        "  # Check if scaling factors were provided\n",
        "  # ASSUMES IDENTICAL SPATIAL SCALING\n",
        "  if 'scales' in kwargs:\n",
        "    (yu, yx, yt) = kwargs['scales']\n",
        "    L_kwargs['scales'] = (yu, yx, yt)\n",
        "\n",
        "    # Rescale each field\n",
        "    for n in range(len(yu)):\n",
        "      U_[n] *= yu[n]\n",
        "\n",
        "  # Check for extra variables\n",
        "  if 'aux_fields' in kwargs:\n",
        "    aux_fields = kwargs['aux_fields']\n",
        "  else:\n",
        "    aux_fields = []\n",
        "  U_ += aux_fields\n",
        "\n",
        "  # Create function names\n",
        "  fj_names = ['poly']*len(fj['poly']) + ['trig']*len(fj['trig'])\n",
        "\n",
        "  # The library is a K x SJ matrix\n",
        "  (K, S, J) = (len(mask[0]), len(alpha)-1, sum(len(fcn) for fcn in fj.values()))\n",
        "  L = torch.zeros(K, S*J, dtype=U[0].dtype)\n",
        "\n",
        "  futures = []\n",
        "  with ProcessPoolExecutor() as executor:\n",
        "    for j in range(1, J+1):\n",
        "      name = fj_names[j-1]\n",
        "      fj_of_U = evaluate_fj(name, fj, j, U_)\n",
        "      for i in range(1, S+1):\n",
        "        futures.append(executor.submit(parallel_worker, i, j, fj_of_U.clone(), name, test_fcns, dX, mask, D, L_kwargs))\n",
        "\n",
        "    for future in as_completed(futures):\n",
        "      result, (i, j) = future.result()\n",
        "      if result is not None:\n",
        "        L[:, (i-1)*J + j-1] = result\n",
        "\n",
        "  cols_to_remove = [len(fj['poly'])*c for c in range(1, S)]\n",
        "  Lib_mask = torch.ones(L.shape[1], dtype=torch.bool)\n",
        "  Lib_mask[cols_to_remove] = False\n",
        "  L = L[:, Lib_mask]\n",
        "\n",
        "  return L"
      ],
      "metadata": {
        "id": "4RdBzxSZj5--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MSTLS Optimization"
      ],
      "metadata": {
        "id": "Gqwkwm-unoXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes loss for a given candidate threshold\n",
        "def loss(w_lambda, w_LS, L):\n",
        "  # Least squares term\n",
        "  LS_num = torch.linalg.norm(torch.matmul(L, w_lambda-w_LS)).item()\n",
        "  LS_denom = torch.linalg.norm(torch.matmul(L, w_LS)).item()\n",
        "  LS_term = LS_num / LS_denom\n",
        "\n",
        "  # Zero norm term\n",
        "  zero_norm = sum(w_lambda != 0).item()/w_lambda.shape[0]\n",
        "\n",
        "  # Return total loss(lambda_n)\n",
        "  loss_n = LS_term + zero_norm\n",
        "  return loss_n"
      ],
      "metadata": {
        "id": "vPUpJrMYmf8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MSTLS inner loop\n",
        "def MSTLS_iterate(L, b, lambda_n, **kwargs):\n",
        "  # kwargs = {w_LS, M, max_its}\n",
        "  #----------------------------------------\n",
        "  # w_LS = precomputed LS solution\n",
        "  # M = scale matrix\n",
        "  # max_its = maximum number of iterations\n",
        "  #----------------------------------------\n",
        "\n",
        "  if 'max_its' in kwargs:\n",
        "    max_its = kwargs['max_its']\n",
        "  else:\n",
        "    max_its = L.shape[1]\n",
        "  if 'M' in kwargs:\n",
        "      M = kwargs['M']\n",
        "\n",
        "  # Check if precomputed LS solution exists\n",
        "  if 'w_LS' in kwargs:\n",
        "    w_LS_ = kwargs['w_LS']\n",
        "  # Otherwise, compute it\n",
        "  else:\n",
        "    w_LS_ = torch.linalg.lstsq(L, b, driver='gelsd').solution\n",
        "\n",
        "  # Undo scaling if necessary\n",
        "  if 'M' in kwargs:\n",
        "    w_for_loss = w_LS_\n",
        "    w_LS_ = torch.matmul(torch.diag(1/torch.diag(M)), w_LS_)\n",
        "\n",
        "  # Compute |b|/|Li| bounds for all columns\n",
        "  norm_b = torch.linalg.norm(b)\n",
        "  norm_Li = torch.linalg.norm(L,dim=0)\n",
        "  bounds = norm_b / norm_Li\n",
        "\n",
        "  # Rescale the bounds if necessary\n",
        "  if 'M' in kwargs:\n",
        "    mu = torch.diag(M)\n",
        "    bounds = bounds / mu\n",
        "\n",
        "  # Define upper and lower bounds (lambda thresholding)\n",
        "  L_bounds = lambda_n*torch.maximum(bounds,torch.ones(bounds.shape[0]))\n",
        "  U_bounds = 1/lambda_n*torch.minimum(bounds,torch.ones(bounds.shape[0]))\n",
        "\n",
        "  # Begin applying iterative thresholding on elements of weight vector\n",
        "  iteration = 0\n",
        "  w_n = w_LS_\n",
        "  inds_old = torch.tensor([0])\n",
        "  while iteration <= max_its:\n",
        "    # Find in-bound and out-of-bound indices and set them to zero\n",
        "    ib_inds = torch.where((abs(w_n[:,0])>=L_bounds)&(abs(w_n[:,0])<=U_bounds))[0]\n",
        "    oob_inds = torch.where((abs(w_n[:,0])<L_bounds)|(abs(w_n[:,0])>U_bounds))[0]\n",
        "\n",
        "    # Check stopping condition\n",
        "    if (torch.equal(inds_old, ib_inds)) or (ib_inds.shape[0]==0):\n",
        "      break\n",
        "\n",
        "    # Find LS solution amongst sparser, in-bound indices\n",
        "    w_n[ib_inds] = torch.linalg.lstsq(L[:,ib_inds], b, driver='gelsd').solution\n",
        "    # Mask oob columns of L\n",
        "    w_n[oob_inds] = 0\n",
        "\n",
        "    # Rescale sparse solution if needed\n",
        "    if 'M' in kwargs:\n",
        "      w_n = torch.matmul(torch.diag(1/torch.diag(M)), w_n)\n",
        "\n",
        "    # Update loop variables\n",
        "    inds_old = ib_inds\n",
        "    iteration += 1\n",
        "    if iteration == max_its:\n",
        "      print('MSTLS reached the maximum number of iterations allowed.')\n",
        "\n",
        "  # Evaluate the loss function on the resulting weights\n",
        "  if 'M' in kwargs:\n",
        "    # Loss evaluated on scaled weights\n",
        "    loss_n = loss(torch.matmul(M,w_n), w_for_loss, L)\n",
        "  else:\n",
        "    # Loss evaluated on input weights (scaled or unscaled)\n",
        "    loss_n = loss(w_n, w_LS_, L)\n",
        "  return w_n, loss_n"
      ],
      "metadata": {
        "id": "f6Ib-H0aTuSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full MSTLS optimization, minimizing loss\n",
        "def MSTLS(L, b, lambdas, **kwargs):\n",
        "  # kwargs = {threshold, w_LS, M, max_its}\n",
        "  #----------------------------------------\n",
        "  # threshold = known optimal threshold\n",
        "  # w_LS = precomputed LS solution\n",
        "  # M = scale matrix (according to paper)\n",
        "  # max_its = maximum number of iterations\n",
        "  #----------------------------------------\n",
        "\n",
        "  if 'max_its' in kwargs:\n",
        "    max_its = kwargs['max_its']\n",
        "  else:\n",
        "    max_its = L.shape[1]\n",
        "  if 'M' in kwargs:\n",
        "    M = kwargs['M']\n",
        "\n",
        "  # If needed, cast L,b to common data type\n",
        "  if b.dtype != L.dtype:\n",
        "    b.to(torch.float64) # LHS\n",
        "    L.to(torch.float64) # RHS\n",
        "\n",
        "  # Check if precomputed LS solution exists\n",
        "  if 'w_LS' in kwargs:\n",
        "    w_LS_ = kwargs['w_LS']\n",
        "  # Otherwise, compute w_LS\n",
        "  else:\n",
        "    w_LS_ = torch.linalg.lstsq(L,b, driver='gelsd').solution\n",
        "\n",
        "  # Check if known optimal threshold was provided\n",
        "  if 'threshold' in kwargs:\n",
        "    lambda_star = kwargs['threshold']\n",
        "\n",
        "  # Otherwise, iterate to find lambda_star\n",
        "  else:\n",
        "    # If multidimensional, iterate over each field\n",
        "    lambda_stars = []\n",
        "\n",
        "    for n in range(w_LS_.shape[1]):\n",
        "      w_LS_n = w_LS_[:,n].reshape(-1,1) # Get a column for a particular field Un\n",
        "      b_n = b[:,n].reshape(-1,1)\n",
        "      loss_history = []\n",
        "\n",
        "      for lambda_n in lambdas:\n",
        "        # Compute candidate weights and evaluate loss\n",
        "        if 'M' in kwargs:\n",
        "          w_n,loss_n = MSTLS_iterate(L,b_n,lambda_n.item(),w_LS=w_LS_n,M=M,max_its=max_its)\n",
        "        else:\n",
        "          w_n,loss_n = MSTLS_iterate(L,b_n,lambda_n.item(),w_LS=w_LS_n,max_its=max_its)\n",
        "        loss_history.append(loss_n)\n",
        "\n",
        "      # Find optimal candidate threshold for each field Un\n",
        "      # (Automatically finds smallest minimzer, if not unique)\n",
        "      ind_star = loss_history.index(min(loss_history))\n",
        "      lambda_star = lambdas[ind_star].item()\n",
        "      lambda_stars.append(lambda_star)\n",
        "\n",
        "  # Use the optimal threshold to compute the final sparse weights\n",
        "  w_stars = []\n",
        "  loss_stars = []\n",
        "\n",
        "  # Again, iterate over each field\n",
        "  for n in range(w_LS_.shape[1]):\n",
        "    w_LS_n = w_LS_[:,n].reshape(-1,1)\n",
        "    b_n = b[:,n].reshape(-1,1)\n",
        "    lambda_star = lambda_stars[n]\n",
        "\n",
        "    if 'M' in kwargs:\n",
        "      w_star_,loss_star_ = MSTLS_iterate(L,b_n,lambda_star,w_LS=w_LS_n,M=M,max_its=max_its)\n",
        "      w_stars.append(w_star_)\n",
        "      loss_stars.append(loss_star_)\n",
        "    else:\n",
        "      w_star_,loss_star_ = MSTLS_iterate(L, b_n, lambda_star, w_LS=w_LS_n, max_its=max_its)\n",
        "      w_stars.append(w_star_)\n",
        "      loss_stars.append(loss_star_)\n",
        "\n",
        "  # Return final result\n",
        "  w_star_ = torch.cat(w_stars, 1)\n",
        "  return w_star_, lambda_stars, loss_stars"
      ],
      "metadata": {
        "id": "4WH69KgtXcSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Print the Results"
      ],
      "metadata": {
        "id": "4aIQxDOJxMgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints a report of WSINDy run\n",
        "\n",
        "def print_report(**kwargs):\n",
        "  # kwargs = {verbosity, thresh, loss_val, L, scales, p, s}\n",
        "  #----------------------------------------\n",
        "  # verbosity = create plots? (0 or 1)\n",
        "  # thresh = optimal MSTLS threshold\n",
        "  # loss_val = minimum MSTLS loss value\n",
        "  # L = model library\n",
        "  # scales = scale factors ([yu's], yx, yt)\n",
        "  # p = test function degrees\n",
        "  # s = query point subsampling rates\n",
        "  #----------------------------------------\n",
        "\n",
        "  if 'verbosity' not in kwargs:\n",
        "    return\n",
        "\n",
        "  elif kwargs['verbosity'] == 1:\n",
        "\n",
        "    # Check if GPU was utilized\n",
        "    if torch.cuda.is_available():\n",
        "      print(f'\\nLibrary computed with CUDA ({torch.cuda.get_device_name()}).')\n",
        "\n",
        "    if 'thresh' and 'loss_val' in kwargs:\n",
        "      # Print optimal MSTLS parameters\n",
        "      (thresh, loss_val) = (kwargs['thresh'], kwargs['loss_val'])\n",
        "      print(f'\\nOptimal threshold = {thresh}\\nOptimal loss = {loss_val}')\n",
        "\n",
        "    if 'L' and 'scales' and 'p' and 's' in kwargs:\n",
        "      # Print the condition number of the library matrix\n",
        "      (L,scales,p,s) = (kwargs['L'],kwargs['scales'],kwargs['p'],kwargs['s'])\n",
        "      print(f'\\nCond(L) = {torch.linalg.cond(L,p=2)}\\n')\n",
        "      print(f'scales = {scales}\\np = {p}\\ns = {s}\\n')\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "5QFDkA0NdXpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a string for each possible term in the model library.\n",
        "# Expects U as a list, U=[U1,...,Un]\n",
        "# NOT SET UP FOR TRIG FCNS YET\n",
        "\n",
        "def get_term_names(U, fj, alpha, D, **kwargs):\n",
        "  # kwargs = {aux_fields}\n",
        "  #-------------------------------------------------\n",
        "  # aux_fields = extra library variables [V1,...,Vm]\n",
        "  #-------------------------------------------------\n",
        "\n",
        "  # Check if extra variables were provided\n",
        "  if 'aux_fields' in kwargs:\n",
        "    aux_fields = kwargs['aux_fields']\n",
        "  else:\n",
        "    aux_fields = []\n",
        "  U_ = U + aux_fields\n",
        "\n",
        "  # Format derivative library as a list of strings\n",
        "  alpha_names = []\n",
        "  for elem in alpha:\n",
        "    if all(value == 0 for value in elem):\n",
        "      alpha_names.append('')\n",
        "    else:\n",
        "      # 1D, 2D, 3D case-handling\n",
        "      if D == 1:\n",
        "        alpha_names.append('_'+'t'*elem[1]+'x'*elem[0])\n",
        "      elif D == 2:\n",
        "        alpha_names.append('_'+'t'*elem[2]+'x'*elem[0]+'y'*elem[1])\n",
        "      elif D == 3:\n",
        "        alpha_names.append('_'+'t'*elem[3]+'x'*elem[0]+'y'*elem[1]+'z'*elem[2])\n",
        "      # General case\n",
        "      else:\n",
        "        string = [('x' + str(n+1) + ',')*elem[n] for n in range(len(elem)-1)]\n",
        "        string = ('_'+'t,'*elem[3] + ''.join(string)).rstrip(',')\n",
        "        alpha_names.append(string)\n",
        "\n",
        "  # Term names (e.g., 'u_t', 'u_x' etc.)\n",
        "  # Case-handling for 1, 2, and 3 fields\n",
        "  if len(U + aux_fields) <= 3:\n",
        "    field_names = ['u', 'v', 'w'][0:len(U_)]\n",
        "  # General case\n",
        "  else:\n",
        "    field_names = ['u' + str(n+1) for n in range(len(U_))]\n",
        "\n",
        "  LHS_names = [str(u) + alpha_names[0] for u in field_names]\n",
        "  RHS = [field_names[n] + '^' + str(fj['poly'][elem][n])\n",
        "         for elem in range(len(fj['poly'])) for n in range(len(U_))]\n",
        "  RHS = [''.join(RHS[i:i+len(U_)]) for i in range(0, len(RHS), len(U_))]\n",
        "  RHS = ['(' + elem + ')' for elem in RHS]\n",
        "  RHS = [u + Di for Di in alpha_names[1:] for u in RHS]\n",
        "\n",
        "  # Remove skipped terms\n",
        "  remove = [len(fj['poly'])*c for c in range(1,len(alpha)-1)]\n",
        "  RHS_names = [term for col, term in enumerate(RHS) if col not in remove]\n",
        "\n",
        "  term_names = (LHS_names, RHS_names)\n",
        "  return term_names"
      ],
      "metadata": {
        "id": "x53N-dry-qjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get discovered PDE model symbolically, as a string\n",
        "\n",
        "def get_model(w, term_names):\n",
        "  # Split into LHS and RHS\n",
        "  (LHS_names, RHS_names) = term_names\n",
        "\n",
        "  # Find nonzero RHS terms\n",
        "  inds = [torch.nonzero(w[:,n])[:,0].tolist() for n in range(w.shape[1])]\n",
        "\n",
        "  # Append terms onto model\n",
        "  model = []\n",
        "  for n in range(w.shape[1]):\n",
        "    terms_n = ['({0:.2f})'.format(w[ind,n].item())+RHS_names[ind] for ind in inds[n]]\n",
        "\n",
        "    # Add to evolution operator\n",
        "    model_n = LHS_names[n] + ' = ' + ' + '.join(terms_n)\n",
        "    model.append(model_n)\n",
        "\n",
        "  # Collapse to a single string and return\n",
        "  model = '\\n'.join(model)\n",
        "  return model"
      ],
      "metadata": {
        "id": "_s-m7VrXMOYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kwarg Handling"
      ],
      "metadata": {
        "id": "Cdkce2AhLvEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kwarg_handling(U, alpha, **kwargs):\n",
        "\n",
        "  # WSINDy Kwarg Handling Function\n",
        "  # This function manages the following hyperparameters.\n",
        "  #-----------------------------------------------------\n",
        "  # x = spatial domain(s)\n",
        "  # dx = spatial discretization(s)\n",
        "  # t = temporal domain\n",
        "  # dt = temporal discretization\n",
        "  # aux_fields = extra variables for term library\n",
        "  #\n",
        "  # m = explicit (mx,...,mt) values\n",
        "  # s = explicit (sx,...,st) values\n",
        "  # lambdas = MSTLS threshold search space\n",
        "  # threshold = known optimal threshold\n",
        "  # p = explicit (px,...,pt) values\n",
        "  # tau = test function tolerance\n",
        "  # tau_hat = Fourier test function tolerance\n",
        "  # scales = explicit (yu,yx,yt) scaling factors\n",
        "  # M = scaling matrix\n",
        "  #\n",
        "  # verbosity = create plots? (0 or 1)\n",
        "  # init_guess = initial guess for (kx,kt) curve fitting\n",
        "  # max_its = specify maximum number of MSTLS iterations\n",
        "  # sigma_NR = noise ratio of artifical gaussian noise\n",
        "  #-----------------------------------------------------\n",
        "\n",
        "  # Dimensionality\n",
        "  #---------------------------------------------------------------------------\n",
        "  # --- Multiple Fields ---\n",
        "  # Check if state vector is multi-dimensional U=(U1,...,Un)\n",
        "  if type(U) is list:\n",
        "    multi_flag = 1 # Multiple fields detected\n",
        "  else:\n",
        "    multi_flag = 0 # Single field detected\n",
        "\n",
        "  # Avoiding some if statements below\n",
        "  # For convenience later on\n",
        "  U_ = [[U], U][multi_flag] # List-ify\n",
        "  dtype_ = U_[0].dtype\n",
        "  Ushape_ = U_[0].shape\n",
        "\n",
        "  # --- Auxiliary Fields ---\n",
        "  # Extra variables to be included in library\n",
        "  if 'aux_fields' in kwargs:\n",
        "    aux_fields = kwargs['aux_fields']\n",
        "  else:\n",
        "    aux_fields = []\n",
        "\n",
        "  # Check that function list {fj} is the correct shape\n",
        "  if (fj['poly'][0] != ()) and (len(fj['poly'][0]) != len(U_+aux_fields)):\n",
        "    raise Exception(\"Nonempty 'fj['poly']' does not match the shape of 'U'.\")\n",
        "\n",
        "  # Check that constant term exists (D^1 = zeros, f1 = zeros)\n",
        "  # TEMPORARY ---> AUTOMATE THIS (???)\n",
        "  if all(fj['poly'][0]) != all(alpha[1]):\n",
        "    raise Exception('Column removal not compatible with given fj, alpha.')\n",
        "\n",
        "  # --- Discretized Grid ---\n",
        "  # Space\n",
        "  if ('x' in kwargs) and ('dx' in kwargs):\n",
        "    (x, dX) = (kwargs['x'], kwargs['dx'])\n",
        "    (D, X) = (len(x), []) # No. and list of spatial variables\n",
        "    for d in range(D):\n",
        "      X.append(torch.arange(x[d][0], x[d][1]+dX[d], dX[d], dtype=dtype_))\n",
        "\n",
        "  elif ('x' in kwargs) ^ ('dx' in kwargs):\n",
        "    raise Exception(\"Both 'x' and 'dx' must be provided simultaneously.\")\n",
        "\n",
        "  else: # Set unit square domain by default\n",
        "    dX = []\n",
        "    X = []\n",
        "    D = U_[0].dim()-1\n",
        "    for d in range(D):\n",
        "      dX.append(1/(Ushape_[d]))\n",
        "      X.append(torch.arange(x[d][0], x[d][1]+dX[d], dX[d], dtype=dtype_))\n",
        "\n",
        "  # Time\n",
        "  if ('t' in kwargs) and ('dt' in kwargs):\n",
        "    (t, dt) = (kwargs['t'], kwargs['dt'])\n",
        "    dX.append(dt)\n",
        "    X.append(torch.arange(t[0], t[1]+dt, dt, dtype=dtype_))\n",
        "\n",
        "  elif ('t' in kwargs) ^ ('dt' in kwargs):\n",
        "    raise Exception(\"Both 't' and 'dt' must be provided simultaneously.\")\n",
        "\n",
        "  else:\n",
        "    dt = 1/(Ushape_[-1])\n",
        "    dX.append(dt)\n",
        "    X.append(torch.arange(0, 1+dt, dt, dtype=dtype_))\n",
        "  #---------------------------------------------------------------------------\n",
        "\n",
        "  # Kwarg Checking\n",
        "  #---------------------------------------------------------------------------\n",
        "  # --- Hyperparameters ---\n",
        "  # Test function support (m)\n",
        "  if 'm' in kwargs:\n",
        "    m = kwargs['m']\n",
        "    m_flag = 0\n",
        "  else:\n",
        "    m_flag = 1 # Automatically compute m\n",
        "\n",
        "  # Query point subsampling rate (s)\n",
        "  if 's' in kwargs:\n",
        "    s = kwargs['s']\n",
        "  else:\n",
        "    s = (int(Ushape_[0]/25),)*D + (int(Ushape_[-1]/25),)\n",
        "\n",
        "  # Threshold search space (lambdas)\n",
        "  if 'lambdas' in kwargs:\n",
        "    lambdas = kwargs['lambdas']\n",
        "  else:\n",
        "    # Default search space\n",
        "    lambdas = 10**((4/49)*torch.arange(0,50)-4)\n",
        "\n",
        "  # Known optimal threshold (threshold)\n",
        "  if 'threshold' in kwargs:\n",
        "    threshold = kwargs['threshold']\n",
        "  else:\n",
        "    threshold = None\n",
        "\n",
        "  # Test function degrees (p)\n",
        "  if 'p' in kwargs:\n",
        "    p = kwargs['p']\n",
        "    p_flag = 0\n",
        "  else:\n",
        "    p_flag = 1 # Automatically compute p\n",
        "\n",
        "  # Test function tolerances (tau, tau_hat)\n",
        "  if 'tau' in kwargs: # Real space\n",
        "    tau = kwargs['tau']\n",
        "  else:\n",
        "    tau = 1E-10\n",
        "  if 'tau_hat' in kwargs: # Fourier\n",
        "    tau_hat = kwargs['tau_hat']\n",
        "  else:\n",
        "    tau_hat = 2\n",
        "\n",
        "  # --- Miscellaneous ---\n",
        "  # Display information and plots\n",
        "  if 'verbosity' in kwargs:\n",
        "    verbosity = kwargs['verbosity']\n",
        "  else:\n",
        "    verbosity = 0\n",
        "\n",
        "  # Curve-fitting initial guess (x, y, slope1, slope2)\n",
        "  if 'init_guess' in kwargs:\n",
        "    init_guess = kwargs['init_guess']\n",
        "    init_guess_x = init_guess[0]\n",
        "    init_guess_t = init_guess[1]\n",
        "  else:\n",
        "    init_guess_x = [15, 1, 10, 0]\n",
        "    init_guess_t = [3, 1, 10, 0.25]\n",
        "\n",
        "  # Maximum inner MSTLS loop iterations\n",
        "  if 'max_its' in kwargs:\n",
        "    max_its = kwargs['verbosity']\n",
        "  else:\n",
        "    max_its = None # Use default value\n",
        "\n",
        "  # Artifical noise\n",
        "  if 'sigma_NR' in kwargs:\n",
        "    # Add noise to data\n",
        "    sigma_NR = kwargs['sigma_NR']\n",
        "\n",
        "    if multi_flag == 0:\n",
        "      U_rms = (torch.sqrt(torch.mean(U**2))).item();\n",
        "      sigma = sigma_NR * U_rms\n",
        "      epsilon = torch.normal(mean=0, std=sigma, size=Ushape_, dtype=dtype_)\n",
        "      U = U + epsilon\n",
        "\n",
        "    else:\n",
        "      U_rms = [(torch.sqrt(torch.mean(u**2))).item() for u in U]\n",
        "      U_noise = []\n",
        "      for n in range(len(U)):\n",
        "        sigma = sigma_NR * U_rms[n]\n",
        "        epsilon = torch.normal(mean=0, std=sigma, size=Ushape_, dtype=dtype_)\n",
        "        U_noise.append(U[n] + epsilon)\n",
        "      U = U_noise\n",
        "\n",
        "    # Re-listify\n",
        "    U_ = [[U], U][multi_flag]\n",
        "  #---------------------------------------------------------------------------\n",
        "\n",
        "  # Automatic Hyperparameter Selection\n",
        "  #---------------------------------------------------------------------------\n",
        "  # Test function support\n",
        "  if m_flag == 1:\n",
        "    # The first field U=(U1,...,Un) will be used to determine m\n",
        "    U1_ = [U, U[0]][multi_flag]\n",
        "    m = []\n",
        "\n",
        "    # Space (mx, my, ...)\n",
        "    m_kwargs = {'verbosity':verbosity, 'init_guess':init_guess_x}\n",
        "    for d in range(D):\n",
        "      m.append(optimal_support(U1_, d, two_piece, F, tau, tau_hat, **m_kwargs))\n",
        "\n",
        "    # Time (mt)\n",
        "    m_kwargs = {'verbosity':verbosity, 'init_guess':init_guess_t}\n",
        "    m.append(optimal_support(U1_, D, two_piece, F, tau, tau_hat, **m_kwargs))\n",
        "\n",
        "  # Test function degrees\n",
        "  if p_flag == 1:\n",
        "    # Solve minimization problem for degree p\n",
        "    p = []\n",
        "    for d in range(D + 1):\n",
        "      alpha_bar = max(tuple(item[d] for item in alpha))\n",
        "      log_tau_term = np.ceil(np.log(tau)/np.log((2*m[d]-1)/m[d]**2))\n",
        "      p.append(max(log_tau_term, alpha_bar + 1))\n",
        "\n",
        "  # Scaling factors (yu, yx, yt) and scaling matrix (M)\n",
        "  # ASSUMES IDENTICAL SPATIAL RESCALING\n",
        "  if ('scales' in kwargs) and ('M' in kwargs):\n",
        "    (scales, M) = (kwargs['scales'], kwargs['M'])\n",
        "  else:\n",
        "    # Automatically compute scales\n",
        "    yu, yx, yt, M = get_scales(fj, alpha, U_, m, dX, p)\n",
        "    scales = (yu, yx, yt)\n",
        "\n",
        "  # Also for convenience later on (for LHS)\n",
        "  LHS_kwargs = [(scales[0][n],)+scales[-2:] for n in range(len(U_))]\n",
        "\n",
        "  # Test function kwargs\n",
        "  tf_dict = {'scales':scales, 'verbosity':verbosity}\n",
        "\n",
        "  # MSTLS kwargs\n",
        "  MSTLS_dict = {'M' : M}\n",
        "  if threshold is not None:\n",
        "    MSTLS_dict['threshold'] = threshold\n",
        "  if max_its is not None:\n",
        "    MSTLS_dict['max_its'] = max_its\n",
        "\n",
        "  # print_report kwargs\n",
        "  print_dict = {'verbosity':verbosity, 'scales':scales, 'p':p, 's':s}\n",
        "  model_dict = {'aux_fields':aux_fields}\n",
        "\n",
        "  # Create query point mask\n",
        "  # The same mask should be applied to all fields U=(U1,...,Un)\n",
        "  mask = query_mask(s, m, X, D, verbosity=verbosity)\n",
        "  #---------------------------------------------------------------------------\n",
        "\n",
        "  # Return algorithm parameters\n",
        "  grid = (U_, D, X, dX, aux_fields)\n",
        "  dicts = (print_dict, tf_dict, LHS_kwargs, MSTLS_dict, model_dict)\n",
        "  hyperparams = (m, lambdas, p, tau, scales, mask)\n",
        "\n",
        "  return grid, dicts, hyperparams"
      ],
      "metadata": {
        "id": "rLFNrnDCDIpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Library Formatting"
      ],
      "metadata": {
        "id": "ufui--QblNtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conveniently formats function library\n",
        "# Increments f1 first, then f2, etc...\n",
        "\n",
        "# NOTE: on/off fields should occur at the end of U=(U1,...) list.\n",
        "\n",
        "def get_poly(powers, fields, **kwargs):\n",
        "  # kwargs = {on_off}\n",
        "  # --------------------------------------------------------------------------\n",
        "  # on_off = m, include last m fields of (U1,...,Un) via an exponent := 0 or 1\n",
        "  # on_off_cross = exlcude cross-terms (U1^a)*(U2^b) for on/off fields?\n",
        "  # --------------------------------------------------------------------------\n",
        "\n",
        "  # Search for on/off fields\n",
        "  if 'on_off' in kwargs:\n",
        "    on_off = kwargs['on_off']\n",
        "  else:\n",
        "    on_off = 0\n",
        "  if 'on_off_cross' in kwargs:\n",
        "    on_off_cross = kwargs['on_off_cross']\n",
        "  else:\n",
        "    on_off_cross = True\n",
        "\n",
        "  # Compute unrestricted powers\n",
        "  pows = list(range(powers+1))\n",
        "  poly = [elem[::-1] for elem in itertools.product(pows, repeat=fields-on_off)]\n",
        "\n",
        "  # Add on/off fields\n",
        "  if on_off_cross == True:\n",
        "    # Include potential cross terms\n",
        "    for m in range(on_off):\n",
        "      poly = [list(elem)+[[0],[1]][n] for n in range(2) for elem in poly]\n",
        "  else:\n",
        "    # Exclude potential cross terms\n",
        "    poly = [list(elem)+(on_off*[0]) for elem in poly]\n",
        "    for m in range(fields-on_off, fields):\n",
        "      on_off_term = fields*[0]\n",
        "      on_off_term[m] = 1\n",
        "      poly.append(on_off_term)\n",
        "\n",
        "  # Return as a tuple\n",
        "  poly = tuple([tuple(elem) for elem in poly])\n",
        "  return poly"
      ],
      "metadata": {
        "id": "Nw8Cl3AUnlwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conveniently formats the derivative library\n",
        "# Increments x partials first, then y partials, etc...\n",
        "# (Currently only supports 1D, 2D, and 3D)\n",
        "\n",
        "def get_alpha(D, pure_derivs, cross_derivs):\n",
        "  rhs = [(D+1)*(0,)]\n",
        "\n",
        "  # GENERALIZE THIS (?)\n",
        "  if D > 3:\n",
        "    raise Exception(\"'alpha_rhs' currently only supports 1D, 2D, and 3D.\")\n",
        "\n",
        "  # Pure derivatives\n",
        "  for i in range(1, pure_derivs+1):\n",
        "    rhs.append(tuple([i]) + D*(0,)) # x partials\n",
        "\n",
        "  if D > 1:\n",
        "    for j in range(1, pure_derivs+1):\n",
        "      rhs.append(tuple([0,j]) + (D-1)*(0,)) # y partials\n",
        "\n",
        "  if D > 2:\n",
        "    for k in range(1, pure_derivs+1):\n",
        "      rhs.append(tuple([0,0,k]) + (D-2)*(0,)) # z partials\n",
        "\n",
        "  # Cross derivatives\n",
        "  if D == 2:\n",
        "    for j in range(1, cross_derivs+1):\n",
        "      for i in range(1, cross_derivs+1):\n",
        "        rhs.append((i, j, 0))\n",
        "\n",
        "  elif D == 3:\n",
        "    for k in range(cross_derivs+1):\n",
        "      for j in range(cross_derivs+1):\n",
        "        for i in range(cross_derivs+1):\n",
        "          if (i,j,k) == (0,0,0):\n",
        "            pass\n",
        "          elif (i,j)==(0,0) or (i,k)==(0,0) or (j,k)==(0,0):\n",
        "            pass\n",
        "          else:\n",
        "            rhs.append((i, j, k, 0))\n",
        "\n",
        "  return tuple(rhs)"
      ],
      "metadata": {
        "id": "jVDSQOX_lU4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weak SINDy"
      ],
      "metadata": {
        "id": "X3GJgwzK185Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wsindy(U, fj, alpha, **kwargs):\n",
        "\n",
        "  # Kwarg Checking and Hyperparameters (see 'kwarg_handling')\n",
        "  #---------------------------------------------------------------------------\n",
        "  # Find WSINDy settings\n",
        "  grid, dicts, hyperparams = kwarg_handling(U, alpha, **kwargs)\n",
        "\n",
        "  # Individual variables/hyperparameters\n",
        "  (U, D, X, dX, aux_fields) = grid\n",
        "  (m, lambdas, p, tau, scales, mask) = hyperparams\n",
        "\n",
        "  # Kwarg dictionaries\n",
        "  (print_dict, tf_dict, LHS_kwargs, MSTLS_dict, model_dict) = dicts\n",
        "  #---------------------------------------------------------------------------\n",
        "\n",
        "  # Test Function Creation\n",
        "  #---------------------------------------------------------------------------\n",
        "  # Compute separable test functions\n",
        "  test_fcns = speedy_test_fcns(m, p, tau, X, dX, alpha, D, **tf_dict)\n",
        "\n",
        "  # LHS test functions\n",
        "  LHS_tf = [test_fcn[0,:] for test_fcn in test_fcns]\n",
        "  #---------------------------------------------------------------------------\n",
        "\n",
        "  # Linear System Creation\n",
        "  #---------------------------------------------------------------------------\n",
        "  # Left-hand side (K x n), where n = no. of fields (U1,...,Un)\n",
        "  b = []\n",
        "  for n in range(len(U)):\n",
        "    b.append(create_b(U[n].clone(), LHS_tf, dX, mask, D, scales=LHS_kwargs[n]))\n",
        "  b = torch.cat(tuple(b), 1)\n",
        "\n",
        "  # Model library (K x SJ), where J = beta_bar^n\n",
        "  L = create_L(U, test_fcns, dX, mask, D, alpha, fj, scales=scales)\n",
        "  #L = create_L_parallel(U, test_fcns, dX, mask, D, alpha, fj, scales=scales)\n",
        "  #---------------------------------------------------------------------------\n",
        "\n",
        "  # MSTLS Optimization\n",
        "  #---------------------------------------------------------------------------\n",
        "  # Pass precomputed w_LS as kwarg to save compute time\n",
        "  w_LS = torch.linalg.lstsq(L, b, driver='gelsd').solution\n",
        "  MSTLS_dict['w_LS'] = w_LS.clone()\n",
        "\n",
        "  # Compute sparse weight vector\n",
        "  w, thresh, loss_val = MSTLS(L, b, lambdas, **MSTLS_dict)\n",
        "  #---------------------------------------------------------------------------\n",
        "\n",
        "  # Print Results\n",
        "  #---------------------------------------------------------------------------\n",
        "  print_dict['L'] = L\n",
        "  (print_dict['loss_val'], print_dict['thresh']) = (loss_val, thresh)\n",
        "  print_report(**print_dict)\n",
        "\n",
        "  # Print discovered PDE\n",
        "  term_names = get_term_names(U, fj, alpha, D, **model_dict)\n",
        "  pde = get_model(w, term_names)\n",
        "  print(f'Discovered model:\\n{pde}')\n",
        "  #---------------------------------------------------------------------------\n",
        "\n",
        "  # Return sparse weight vector\n",
        "  return w"
      ],
      "metadata": {
        "id": "Fe8y9nua2BrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Driver Code"
      ],
      "metadata": {
        "id": "-1zB14NOb_wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid parameters (should match dimension of dataset)\n",
        "(Lx, Ly, T) = (30*np.pi, 30*np.pi, 20) # Example\n",
        "(dx, dy, dt) = (Lx/U.shape[0], Ly/U.shape[1], T/U.shape[-1])\n",
        "\n",
        "# Function library\n",
        "fields = 1 # Number of scalar fields\n",
        "powers = 4 # Maximum monomial power\n",
        "poly = get_poly(powers, fields)\n",
        "trig = () # (Frequency, phase) pairs\n",
        "fj = {'poly': poly, 'trig': trig}\n",
        "\n",
        "# Derivative library\n",
        "lhs = ((0,0,1),) # Evolution operator D^0\n",
        "dimension = 2 # Spatial dimensions\n",
        "pure_derivs = 4 # Include up to fourth order\n",
        "cross_derivs = 2 # Include up to second order\n",
        "rhs = get_alpha(dimension, pure_derivs, cross_derivs)\n",
        "alpha = lhs + rhs\n",
        "\n",
        "params = {\n",
        "    # x = spatial domain(s)\n",
        "    # dx = spatial discretization(s)\n",
        "    # t = temporal domain\n",
        "    # dt = temporal discretization\n",
        "    # aux_fields = extra variables\n",
        "    #-------------------------------\n",
        "    'x' : [(0, Lx), (0, Ly)],\n",
        "    'dx' : [dx, dy],\n",
        "    't' : (0, T),\n",
        "    'dt' : dt,\n",
        "\n",
        "    # m = explicit (mx,...,mt) values\n",
        "    # lambdas = MSTLS threshold search space\n",
        "    # threshold = known optimal threshold\n",
        "    # p = explicit (px,...,pt) values\n",
        "    # tau = test function tolerance\n",
        "    # tau_hat = Fourier test function tolerance\n",
        "    # scales = explicit (yu,yx,yt) scaling factors\n",
        "    # M = explicit scaling matrix\n",
        "    #---------------------------------------------\n",
        "\n",
        "    # verbosity = report info and create plots? (0 or 1)\n",
        "    # init_guess = [x0, y0, m1, m2], for (kx,kt) curve fit\n",
        "    # max_its = specify maximum number of MSTLS iterations\n",
        "    # sigma_NR = noise ratio of artifical gaussian noise\n",
        "    #-----------------------------------------------------\n",
        "    'verbosity' : 1,\n",
        "    'sigma_NR' : 0.0}\n",
        "\n",
        "# Run weak SINDy\n",
        "w = wsindy(U, fj, alpha, **params)"
      ],
      "metadata": {
        "id": "Iwa2Bv-nV40Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wjPRmnVpblPk",
        "296iPuVwe1cX",
        "4lr29C1Uebcm",
        "yMtEJdjyYah7",
        "p14j807DZRsF",
        "5XjeC_7wGVEv",
        "Gqwkwm-unoXi",
        "4aIQxDOJxMgu",
        "Cdkce2AhLvEL",
        "ufui--QblNtu",
        "X3GJgwzK185Y"
      ],
      "authorship_tag": "ABX9TyMeOBiBUklIUqhZSMceNaQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}